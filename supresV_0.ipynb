{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "supresV_0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamshikrishna1370/Image_processing_kriti_2017/blob/master/supresV_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdLh4fCmwlY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ae098d1a-fc57-4642-972a-cfa990d2fba2"
      },
      "source": [
        "import random\n",
        "import glob\n",
        "import subprocess\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "run = wandb.init(project='superres',name=\"InitnFinal5X5_adam_mse\")#,run_id='hello')\n",
        "# wandb.create_run(project='superres', username='vkreddy', run_id='Conv_size_InF_5')\n",
        "# api = wandb.Api()\n",
        "# run = api.create_run(\"vkreddy/superres/conv_sizeInF_5\")\n",
        "config = run.config\n",
        "\n",
        "\n",
        "val_dir = 'data/test'\n",
        "train_dir = 'data/train'\n",
        "\n",
        "# automatically get the data if it doesn't exist\n",
        "if not os.path.exists(\"data\"):\n",
        "    print(\"Downloading flower dataset...\")\n",
        "    subprocess.check_output(\n",
        "        \"mkdir data && curl https://storage.googleapis.com/wandb/flower-enhance.tar.gz | tar xzf - -C data\", shell=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/vkreddy/superres/runs/9pe7lwsk\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
              "        in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbdaxbOl5OYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install wandb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBVKSCeY0idC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_generator(batch_size, img_dir):\n",
        "    \"\"\"A generator that returns small images and large images.  DO NOT ALTER the validation set\"\"\"\n",
        "    input_filenames = glob.glob(img_dir + \"/*-in.jpg\")\n",
        "    counter = 0\n",
        "    random.shuffle(input_filenames)\n",
        "    while True:\n",
        "        small_images = np.zeros(\n",
        "            (batch_size, config.input_width, config.input_height, 3))\n",
        "        large_images = np.zeros(\n",
        "            (batch_size, config.output_width, config.output_height, 3))\n",
        "        if counter+batch_size >= len(input_filenames):\n",
        "            counter = 0\n",
        "        for i in range(batch_size):\n",
        "            img = input_filenames[counter + i]\n",
        "            small_images[i] = np.array(Image.open(img)) / 255.0\n",
        "            large_images[i] = np.array(\n",
        "                Image.open(img.replace(\"-in.jpg\", \"-out.jpg\"))) / 255.0\n",
        "        yield (small_images, large_images)\n",
        "        counter += batch_size\n",
        "\n",
        "\n",
        "def perceptual_distance(y_true, y_pred):\n",
        "    \"\"\"Calculate perceptual distance, DO NOT ALTER\"\"\"\n",
        "    y_true *= 255\n",
        "    y_pred *= 255\n",
        "    rmean = (y_true[:, :, :, 0] + y_pred[:, :, :, 0]) / 2\n",
        "    r = y_true[:, :, :, 0] - y_pred[:, :, :, 0]\n",
        "    g = y_true[:, :, :, 1] - y_pred[:, :, :, 1]\n",
        "    b = y_true[:, :, :, 2] - y_pred[:, :, :, 2]\n",
        "\n",
        "    return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)))\n",
        "\n",
        "\n",
        "class ImageLogger(Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        preds = self.model.predict(in_sample_images)\n",
        "        in_resized = []\n",
        "        for arr in in_sample_images:\n",
        "            # Simple upsampling\n",
        "            in_resized.append(arr.repeat(8, axis=0).repeat(8, axis=1))\n",
        "        wandb.log({\n",
        "            \"examples\": [wandb.Image(np.concatenate([in_resized[i] * 255, o * 255, out_sample_images[i] * 255], axis=1)) for i, o in enumerate(preds)]\n",
        "        }, commit=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXhZFykywnhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.num_epochs = 50\n",
        "config.batch_size = 64\n",
        "config.input_height = 32\n",
        "config.input_width = 32\n",
        "config.output_height = 256\n",
        "config.output_width = 256\n",
        "\n",
        "config.steps_per_epoch = len(\n",
        "    glob.glob(train_dir + \"/*-in.jpg\")) // config.batch_size\n",
        "config.val_steps_per_epoch = len(\n",
        "    glob.glob(val_dir + \"/*-in.jpg\")) // config.batch_size\n",
        "\n",
        "\n",
        "val_generator = image_generator(config.batch_size, val_dir)\n",
        "in_sample_images, out_sample_images = next(val_generator)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YZBlLBw1J0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "2372e5ca-c2ce-4115-ba1e-4a744ad99401"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Conv2D(3, (5, 5), activation='relu', padding='same',\n",
        "                        input_shape=(config.input_width, config.input_height, 3)))\n",
        "model.add(layers.UpSampling2D())\n",
        "model.add(layers.Conv2D(3, (3, 3), activation='relu', padding='same'))\n",
        "model.add(layers.UpSampling2D())\n",
        "model.add(layers.Conv2D(3, (3, 3), activation='relu', padding='same'))\n",
        "model.add(layers.UpSampling2D())\n",
        "model.add(layers.Conv2D(3, (5, 5), activation='relu', padding='same'))\n",
        "\n",
        "# DONT ALTER metrics=[perceptual_distance]\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0812 17:42:56.272908 139682311153536 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ct5ji0uxUer",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "6c3682c3-07ff-4574-da60-e1ef826d8343"
      },
      "source": [
        "model.compile(optimizer='adam', loss='mse',\n",
        "              metrics=[perceptual_distance])\n",
        "\n",
        "model.fit_generator(image_generator(config.batch_size, train_dir),\n",
        "                    steps_per_epoch=config.steps_per_epoch,\n",
        "                    epochs=config.num_epochs, callbacks=[\n",
        "                        ImageLogger(), WandbCallback()],\n",
        "                    validation_steps=config.val_steps_per_epoch,\n",
        "                    validation_data=val_generator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "156/156 [==============================] - 21s 135ms/step - loss: 0.0887 - perceptual_distance: 183.9177 - val_loss: 0.0393 - val_perceptual_distance: 126.7149\n",
            "Epoch 2/50\n",
            "156/156 [==============================] - 18s 118ms/step - loss: 0.0342 - perceptual_distance: 116.3130 - val_loss: 0.0313 - val_perceptual_distance: 111.6615\n",
            "Epoch 3/50\n",
            "156/156 [==============================] - 19s 119ms/step - loss: 0.0285 - perceptual_distance: 106.0588 - val_loss: 0.0271 - val_perceptual_distance: 103.6651\n",
            "Epoch 4/50\n",
            "156/156 [==============================] - 19s 120ms/step - loss: 0.0255 - perceptual_distance: 99.9221 - val_loss: 0.0250 - val_perceptual_distance: 99.3064\n",
            "Epoch 5/50\n",
            "156/156 [==============================] - 19s 120ms/step - loss: 0.0239 - perceptual_distance: 96.3111 - val_loss: 0.0237 - val_perceptual_distance: 96.1363\n",
            "Epoch 6/50\n",
            "156/156 [==============================] - 19s 120ms/step - loss: 0.0228 - perceptual_distance: 93.7355 - val_loss: 0.0229 - val_perceptual_distance: 94.1910\n",
            "Epoch 7/50\n",
            "156/156 [==============================] - 18s 118ms/step - loss: 0.0221 - perceptual_distance: 91.8791 - val_loss: 0.0223 - val_perceptual_distance: 92.7654\n",
            "Epoch 8/50\n",
            "156/156 [==============================] - 19s 120ms/step - loss: 0.0216 - perceptual_distance: 90.5166 - val_loss: 0.0219 - val_perceptual_distance: 91.6490\n",
            "Epoch 9/50\n",
            "156/156 [==============================] - 19s 120ms/step - loss: 0.0213 - perceptual_distance: 89.4891 - val_loss: 0.0216 - val_perceptual_distance: 90.7719\n",
            "Epoch 10/50\n",
            "156/156 [==============================] - 19s 120ms/step - loss: 0.0210 - perceptual_distance: 88.7049 - val_loss: 0.0214 - val_perceptual_distance: 90.1202\n",
            "Epoch 11/50\n",
            "156/156 [==============================] - 19s 119ms/step - loss: 0.0208 - perceptual_distance: 88.0750 - val_loss: 0.0213 - val_perceptual_distance: 89.6823\n",
            "Epoch 12/50\n",
            "156/156 [==============================] - 19s 122ms/step - loss: 0.0207 - perceptual_distance: 87.4650 - val_loss: 0.0211 - val_perceptual_distance: 88.9467\n",
            "Epoch 13/50\n",
            "156/156 [==============================] - 19s 121ms/step - loss: 0.0205 - perceptual_distance: 86.8702 - val_loss: 0.0209 - val_perceptual_distance: 88.4375\n",
            "Epoch 14/50\n",
            "156/156 [==============================] - 19s 119ms/step - loss: 0.0204 - perceptual_distance: 86.5124 - val_loss: 0.0208 - val_perceptual_distance: 88.0728\n",
            "Epoch 15/50\n",
            "156/156 [==============================] - 19s 121ms/step - loss: 0.0203 - perceptual_distance: 86.2587 - val_loss: 0.0208 - val_perceptual_distance: 87.8663\n",
            "Epoch 16/50\n",
            "156/156 [==============================] - 19s 120ms/step - loss: 0.0202 - perceptual_distance: 86.0706 - val_loss: 0.0207 - val_perceptual_distance: 87.6937\n",
            "Epoch 17/50\n",
            "156/156 [==============================] - 19s 119ms/step - loss: 0.0202 - perceptual_distance: 85.9017 - val_loss: 0.0207 - val_perceptual_distance: 87.5507\n",
            "Epoch 18/50\n",
            "155/156 [============================>.] - ETA: 0s - loss: 0.0201 - perceptual_distance: 85.7753"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFH_MpxQ4VwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}